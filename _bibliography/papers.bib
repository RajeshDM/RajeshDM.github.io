---
---

@string{aps = {American Physical Society,}}

@article{mangannavar2024hierarchical,
  title={Hierarchical Object-Oriented POMDP Planning for Object Rearrangement},
  author={Mangannavar, Rajesh and Fern, Alan and Tadepalli, Prasad},
  journal={arXiv preprint arXiv:2412.01348},
  year={2024},
  url={https://arxiv.org/pdf/2412.01348},
  selected={true},
  pdf={HOO_POMDP.pdf},
  dimensions={true},
  abbr={Arxiv}
}

@article{mangannavar2024gabar,
  title={GABAR: Graph Attention-Based Action Ranking for Relational Policy Learning},
  author={Mangannavar, Rajesh and Lee, Stefan and Fern, Alan and Tadepalli, Prasad},
  journal={arXiv preprint arXiv:2412.04752},
  year={2024},
  url={https://arxiv.org/pdf/2412.04752},
  selected={true},
  pdf={GABAR.pdf},
  dimensions={true},
  abbr={Arxiv}
}

@article{mangannavar2019learning,
abbr={ISNN},
title={Learning Agents with Prioritization and Parameter Noise in Continuous State and Action Space},
author={Mangannavar, R. and Srinivasaraghavan, G.},
abstract={Among the many variants of RL, an important class of problems is where the state and action spaces are continuous---autonomous robots, autonomous vehicles, optimal control are all examples of such problems that can lend themselves naturally to reinforcement based algorithms, and have continuous state and action spaces. In this paper, we introduce a prioritized form of a combination of state-of-the-art approaches such as Deep Q-learning (DQN) and Deep Deterministic Policy Gradient (DDPG) to outperform the earlier results for continuous state and action space problems. Our experiments also involve the use of parameter noise during training resulting in more robust deep RL models outperforming the earlier results significantly. We believe these results are a valuable addition for continuous state and action space problems.},
journal={Advances in Neural Networks -- ISNN},
volume={11731},
pages={202--212},
numpages={11},
year={2019},
publisher={Springer International Publishing},
doi={10.1007/978-3-030-22796-8_22},
url={https://doi.org/10.1007/978-3-030-22796-8_22},
html={https://doi.org/10.1007/978-3-030-22796-8_22},
pdf={mangannavar2019learning.pdf},
dimensions={true},
selected={true},
pdf={PDDPG.pdf}
}

@article{mangannavar2025planning,
  title={Planning with affordances: Integrating learned affordance models and symbolic planning},
  author={Mangannavar, Rajesh},
  journal={arXiv preprint arXiv:2502.02768},
  year={2025},
  url={https://arxiv.org/pdf/2502.02768},
  selected={true},
  pdf={Planned_affordances.pdf},
  dimensions={true},
  abbr={Arxiv}
}